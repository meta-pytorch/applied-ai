"""
# Test parameters
        G = 2  # Number of groups
        M = 256  # Input dimension
        N = 128  # Output dimension per group
        K = 64  # Hidden dimension
Successful run:
2025-03-09 22:56:57,425 - INFO - grad W compare: grad_w=tensor([[  5.1562,  -1.5703,  -3.1719,  ..., -14.8750,  -7.4062,  24.5000],
        [  2.2500,  17.0000,  -9.4375,  ..., -10.0625,  10.6875,  12.6875],
        [-10.7500,   3.3281,  -4.8438,  ...,   1.6484,  -3.1562,  -7.5938],
        ...,
        [  8.6250,   1.6641,  14.2500,  ..., -24.5000,   0.4570,  -4.3750],
        [ 11.1875,  23.5000,   7.7500,  ...,  -9.1875, -14.9375,  -9.8750],
        [  2.1250,  -6.7188, -16.0000,  ...,  -7.1875, -12.8125,  17.8750]],
       device='cuda:0', dtype=torch.bfloat16), w_autograd=tensor([[ 0.8633,  0.4434,  2.2656,  ..., -1.0859,  1.2266, -0.2812],
        [ 0.8477, -0.2734,  1.6719,  ..., -0.7891,  0.6953,  1.4844],
        [ 0.4434, -0.5547, -0.4590,  ...,  0.4531,  0.3242,  0.9258],
        ...,
        [-0.1074, -0.4102, -0.5547,  ...,  0.4297, -0.1011, -2.4219],
        [-1.0859,  0.3730,  0.9141,  ...,  0.8516,  0.2285,  0.5859],
        [-1.2266,  1.7578,  0.7852,  ...,  0.5977,  1.4609, -1.4453]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
2025-03-09 22:56:57,427 - INFO - grad X compare: grad_x=tensor([[-20.5000,  -3.4219,  13.5000,  ...,   2.5781,  -6.5000, -25.2500],
        [  6.1875,  -5.9375,  -6.0312,  ...,   6.4375,   6.5312,  -8.3750],
        [-10.3750,  -5.5312,   1.3672,  ..., -13.5000,   4.0938,  -8.0625],
        ...,
        [ 21.3750,  -7.7812,  -0.1128,  ...,  -4.2188,   3.5781,  23.0000],
        [ -5.9688,   9.8125,  -2.2812,  ...,   7.2500,   6.1250,  -2.8438],
        [ -3.2031,   1.8125, -21.2500,  ..., -18.6250,   8.6875,  11.3125]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<CopySlices>), x_autograd=tensor([[-0.2539, -0.3145, -1.8047,  ..., -0.6641,  0.8086, -0.9023],
        [-0.0610, -1.8438, -0.8281,  ...,  1.6250, -1.0469, -0.0261],
        [-0.0127, -0.2969, -0.2598,  ...,  0.4609, -1.4453, -1.3672],
        ...,
        [ 1.1562, -0.1836,  1.0312,  ..., -0.6328,  0.7305,  0.3047],
        [-0.2471, -1.2812,  0.2715,  ...,  0.0019, -0.6172,  0.7188],
        [ 0.9141, -0.6992, -0.8750,  ...,  0.3223, -1.1406, -1.0859]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
2025-03-09 22:56:57,427 - INFO - Maximum gradient error - grad_x: 0.0078125, grad_w: 0.0
2025-03-09 22:56:57,427 - INFO - ✓ Gradients match the PyTorch reference
2025-03-09 22:56:57,427 - INFO - Test succeeded
"""

# However, the following run failed:
"""
# Test parameters
        G = 2  # Number of groups
        M = 256  # Input dimension
        N = 128  # Output dimension per group
        K = 128  # Hidden dimension

2025-03-09 22:59:11,084 - INFO - Computing grad_w with triton kernel
2025-03-09 22:59:11,424 - INFO - grad_w computation successful with triton
2025-03-09 22:59:11,424 - INFO - Gradient shapes - grad_x: torch.Size([256, 128]), grad_w: torch.Size([256, 128])
2025-03-09 22:59:11,424 - INFO - Running PyTorch reference implementation
2025-03-09 22:59:11,709 - INFO - Comparing gradients with PyTorch reference
2025-03-09 22:59:11,797 - INFO - grad W compare: grad_w=tensor([[-16.6250,  30.3750,  11.5625,  ...,   3.4531,  -0.0559,  -8.1875],
        [  3.2500,  -0.4961,  -5.7500,  ...,  14.1250,   0.4922,   0.1299],
        [  8.5625, -10.3125, -12.0625,  ...,   9.8125,  -3.3281,  17.2500],
        ...,
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],
       device='cuda:0', dtype=torch.bfloat16), w_autograd=tensor([[-0.6992,  0.5117,  1.6328,  ...,  2.2656,  2.7500, -0.5391],
        [ 0.2295, -0.2236, -0.8281,  ...,  1.4844, -0.9922, -0.6250],
        [-0.7773, -2.1250, -0.2832,  ...,  0.8164, -0.1357,  1.8438],
        ...,
        [ 0.1992, -3.0469,  0.6562,  ...,  1.7422,  1.0234, -1.0312],
        [ 1.3281,  1.2031,  1.1172,  ...,  0.3320, -1.0312,  0.5820],
        [ 1.5391,  1.4766, -1.3828,  ...,  0.3145, -1.1875, -0.4629]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
2025-03-09 22:59:11,799 - INFO - grad X compare: grad_x=tensor([[-16.3750,  11.6875,  -1.7891,  ...,  -6.6875,   4.0000,  -4.6875],
        [ -7.7812,  -0.5781,   2.3750,  ...,  14.3125,   7.8438,   0.6484],
        [  0.4102,  14.2500,   9.0625,  ...,  -1.7578,  -0.6992, -19.1250],
        ...,
        [ 20.8750,  14.4375,   9.4375,  ..., -18.1250,   5.5625,   9.7500],
        [-29.1250, -13.9375,  15.7500,  ...,  11.3750,  27.8750,  -5.2188],
        [  0.4551,  -9.0000,   6.8438,  ...,  17.8750,  -5.9375,  -5.5625]],
       device='cuda:0', dtype=torch.bfloat16, grad_fn=<CopySlices>), x_autograd=tensor([[-1.0547,  1.4141,  0.0986,  ...,  1.1484,  0.2852, -0.3672],
        [ 0.1719,  1.7578,  0.7305,  ...,  0.3418, -0.3652, -0.1816],
        [-0.1641, -0.1523, -1.3047,  ...,  0.7148, -0.0623, -1.1250],
        ...,
        [-1.5156,  0.1533,  1.2109,  ..., -0.9961, -0.4648, -0.1235],
        [ 1.1250, -0.5781,  0.3320,  ..., -0.1748,  0.7422, -0.1592],
        [-0.7266,  1.8672, -1.0547,  ..., -0.2891, -0.0952,  0.8594]],
       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)
2025-03-09 22:59:11,799 - INFO - Maximum gradient error - grad_x: 0.00048828125, grad_w: 46.5
2025-03-09 22:59:11,799 - ERROR - ✗ Gradient mismatch above tolerance threshold
"""
